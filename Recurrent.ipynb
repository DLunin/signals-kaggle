{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d4905db9-005f-42f0-aa6b-1408acef7371",
    "_uuid": "4c065a37dd33e869d93ccd8d78daed628e58112b"
   },
   "source": [
    "<a id=\"loading_data\"></a>\n",
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "_cell_guid": "5abea3ac-4fa5-4c4f-893f-7f2afa49e523",
    "_kg_hide-output": true,
    "_uuid": "337e0950ca948be32d5d881c1a3c675ccf7ac523"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1001)\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import pickler\n",
    "\n",
    "import IPython\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import IPython.display as ipd\n",
    "import wave\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "import scipy\n",
    "from keras import losses, models, optimizers\n",
    "from keras.activations import relu, softmax\n",
    "from keras.callbacks import (EarlyStopping, LearningRateScheduler,\n",
    "                             ModelCheckpoint, TensorBoard, ReduceLROnPlateau)\n",
    "from keras.layers import (Convolution1D, Dense, Dropout, GlobalAveragePooling1D, \n",
    "                          GlobalMaxPool1D, Input, MaxPool1D, concatenate, LSTM, GRU, Reshape, ConvLSTM2D,\n",
    "                         TimeDistributed)\n",
    "from keras.utils import Sequence, to_categorical\n",
    "from keras.layers import (Convolution2D, GlobalAveragePooling2D, BatchNormalization, Flatten,\n",
    "                          GlobalMaxPool2D, MaxPool2D, concatenate, Activation)\n",
    "from keras.utils import Sequence, to_categorical\n",
    "from keras import backend as K\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "%matplotlib inline\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "_cell_guid": "97700e3e-82e1-4ce2-9da4-3f8f264e7558",
    "_kg_hide-output": true,
    "_uuid": "2ca1929548de57afb1c4fde19c10f7b18c64264e"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../input/freesound-audio-tagging/train.csv\")\n",
    "test = pd.read_csv(\"../input/freesound-audio-tagging/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "_cell_guid": "e0836104-1a4d-485d-9cc1-3e5b82f449de",
    "_uuid": "66640745984135b853d36eac127fb2da302319ad"
   },
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    def __init__(self,\n",
    "                 sampling_rate=16000, audio_duration=2, n_classes=41,\n",
    "                 use_mfcc=False, n_folds=10, learning_rate=0.0001, \n",
    "                 max_epochs=50, n_mfcc=20):\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.audio_duration = audio_duration\n",
    "        self.n_classes = n_classes\n",
    "        self.use_mfcc = use_mfcc\n",
    "        self.n_mfcc = n_mfcc\n",
    "        self.n_folds = n_folds\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_epochs = max_epochs\n",
    "\n",
    "        self.audio_length = self.sampling_rate * self.audio_duration\n",
    "        if self.use_mfcc:\n",
    "            self.dim = (self.n_mfcc, 1 + int(np.floor(self.audio_length/512)), 1)\n",
    "        else:\n",
    "            self.dim = (self.audio_length, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(sampling_rate=16000, audio_duration=2, n_folds=3, learning_rate=0.001, n_mfcc=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(config, directory, meta=None):\n",
    "    if meta is not None and 'fname' in meta:\n",
    "        meta = meta.copy().set_index('fname')\n",
    "    result = []\n",
    "    label = []\n",
    "    verified = []\n",
    "    for filename in tqdm(os.listdir(directory)):\n",
    "        if not filename.endswith('.wav'):\n",
    "            continue\n",
    "        path = os.path.join(directory, filename)\n",
    "        y, sr = librosa.core.load(path, sr=config.sampling_rate)\n",
    "        result.append(y)\n",
    "        if meta is not None:\n",
    "            label.append(meta.loc[filename].label)\n",
    "            verified.append(meta.loc[filename].manually_verified)\n",
    "    return result, label, verified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dumpit\n",
    "TrainDataX, TrainDataY, TrainDataMV = load(config, '../input/freesound-audio-tagging/audio_train', train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainDataFnames = os.listdir('../input/freesound-audio-tagging/audio_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dumpit\n",
    "TestDataX, _, _ = load(config, '../input/freesound-audio-tagging/audio_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestDataFnames = os.listdir('../input/freesound-audio-tagging/audio_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainDataX = np.asarray(TrainDataX)\n",
    "TrainDataY = np.asarray(TrainDataY)\n",
    "TrainDataMV = np.asarray(TrainDataMV)\n",
    "TrainDataFnames = np.asarray(TrainDataFnames)\n",
    "\n",
    "TestDataX = np.asarray(TestDataX)\n",
    "TestDataFnames = np.asarray(TestDataFnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "def random_id(N=5):\n",
    "    return ''.join(random.choice(string.ascii_lowercase + string.digits) for _ in range(N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    def __init__(self, config, preprocessing_fn=lambda x: x, use_mfcc=False):\n",
    "        self.config = config\n",
    "        self.preprocessing_fn = preprocessing_fn\n",
    "        self.dim = self.config.dim\n",
    "        \n",
    "        self.use_mfcc = use_mfcc\n",
    "        \n",
    "        if self.use_mfcc:\n",
    "            self.dim = (config.n_mfcc, 1 + int(np.floor(config.audio_length/512)), 1)\n",
    "        else:\n",
    "            self.dim = (config.audio_length, 1)\n",
    "        \n",
    "        self.label_idx = {label: i for i, label in enumerate(LABELS)}\n",
    "\n",
    "    def process(self, X, y=None):\n",
    "        res_X = np.empty((len(X), *self.dim))\n",
    "        \n",
    "        res_y = None\n",
    "        if y is not None:\n",
    "            res_y = np.empty(len(X))\n",
    "        else:\n",
    "            y = np.zeros(len(X))\n",
    "        input_length = self.config.audio_length\n",
    "        for i, (data, answer) in tqdm(enumerate(zip(X, y)), total=len(X)):\n",
    "            if len(data) > input_length:\n",
    "                max_offset = len(data) - input_length\n",
    "                offset = np.random.randint(max_offset)\n",
    "                data = data[offset:(input_length+offset)]\n",
    "            else:\n",
    "                if input_length > len(data):\n",
    "                    max_offset = input_length - len(data)\n",
    "                    offset = np.random.randint(max_offset)\n",
    "                else:\n",
    "                    offset = 0\n",
    "                data = np.pad(data, (offset, input_length - len(data) - offset), \"constant\")\n",
    "                \n",
    "            if self.use_mfcc:\n",
    "                data = librosa.feature.mfcc(data, sr=self.config.sampling_rate,\n",
    "                                                   n_mfcc=self.config.n_mfcc)\n",
    "                data = np.expand_dims(data, axis=-1)\n",
    "            else:\n",
    "                data = self.preprocessing_fn(data)[:, np.newaxis]\n",
    "            res_X[i,] = data\n",
    "        \n",
    "            if res_y is not None:\n",
    "                res_y[i] = self.label_idx[answer]\n",
    "        if res_y is not None:\n",
    "            return res_X, to_categorical(res_y, num_classes=self.config.n_classes)\n",
    "        return res_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "49a23330-291d-4eb7-aeb9-4abcfd648277",
    "_uuid": "6b69d10980c7aad004c6a7fa860c649d0b875a0f"
   },
   "source": [
    "<a id=\"1d_normalization\"></a>\n",
    "#### Normalization\n",
    "\n",
    "Normalization is a crucial preprocessing step. The simplest method is rescaling the range of features to scale the range in [0, 1]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "_cell_guid": "bb5936dd-5fb1-4894-8165-6daf372a6832",
    "_uuid": "c9db10ad526815730a6e5a1f057de8c9bff12615"
   },
   "outputs": [],
   "source": [
    "def audio_norm(data):\n",
    "    max_data = np.max(data)\n",
    "    min_data = np.min(data)\n",
    "    data = (data-min_data)/(max_data-min_data+1e-6)\n",
    "    return data-0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3b9656b0-31d3-47ea-9bb3-789a40026793",
    "_uuid": "c2f0bbd810926b309d3b02473e937a2a86bc9005"
   },
   "source": [
    "* The dummy model is just for debugging purpose.\n",
    "* Our 1D Conv model is fairly deep and is trained using Adam Optimizer with a learning rate of 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "_cell_guid": "e9027035-0e77-47dd-8616-113c1cfb37e0",
    "_uuid": "53aca10261dea0b8357e39adb513c7689b7c07ff"
   },
   "outputs": [],
   "source": [
    "LABELS = list(train.label.unique())\n",
    "label_idx = {label: i for i, label in enumerate(LABELS)}\n",
    "train.set_index(\"fname\", inplace=True)\n",
    "test.set_index(\"fname\", inplace=True)\n",
    "train[\"label_idx\"] = train.label.apply(lambda x: label_idx[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc = Preprocessor(config, preprocessing_fn=audio_norm, use_mfcc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5423162c9985419f918873be5ae42b10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9473), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ProcessedTrainDataX, ProcessedTrainDataY = preproc.process(TrainDataX, TrainDataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47825e6bbcab43b2b06a4db5d1b5d5bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9400), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ProcessedTestDataX = preproc.process(TestDataX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9400, 50, 63, 1)"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ProcessedTestDataX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9473, 50, 63, 1)"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ProcessedTrainDataX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test, F_train, F_test = \\\n",
    "    train_test_split(ProcessedTrainDataX, ProcessedTrainDataY, TrainDataFnames, random_state=1337, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelRNN:\n",
    "    def __init__(self, config, learning_rate=0.001, output_dir='output'):\n",
    "        self.config = config\n",
    "        self.learning_rate = learning_rate\n",
    "        self.dim = (config.n_mfcc, 1 + int(np.floor(config.audio_length/512)), 1)\n",
    "        self.model = self._build_model()\n",
    "        self.id = random_id()\n",
    "        self.output_dir = os.path.join(output_dir, self.__class__.__name__ + '_' + self.id)\n",
    "        os.makedirs(self.output_dir)\n",
    "    \n",
    "    def _build_model(self):\n",
    "        nclass = self.config.n_classes\n",
    "        input_length = self.config.audio_length\n",
    "\n",
    "        \n",
    "        inp = Input(shape=(1, self.dim[0], self.dim[1]))\n",
    "        x = inp\n",
    "\n",
    "        #x = Convolution2D(32, (4,10), padding=\"same\")(inp)\n",
    "        #x = BatchNormalization()(x)\n",
    "        #x = Activation(\"relu\")(x)\n",
    "        #x = MaxPool2D()(x)\n",
    "        #x = Dropout(0.1)(x)\n",
    "        \n",
    "        print(x.shape)\n",
    "        \n",
    "        x = Convolution2D(32, (4,10), padding=\"same\", data_format='channels_first')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = MaxPool2D(data_format='channels_first')(x)\n",
    "        x = Dropout(0.1)(x)\n",
    "        \n",
    "        print(x.shape)\n",
    "        \n",
    "        x = TimeDistributed(GRU(32, activation='linear', return_sequences=True))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = MaxPool2D(data_format='channels_first')(x)\n",
    "        x = Dropout(0.1)(x)\n",
    "        \n",
    "        print(x.shape)\n",
    "        \n",
    "        x = Convolution2D(32, (4,10), padding=\"same\", data_format='channels_first')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = MaxPool2D(data_format='channels_first')(x)\n",
    "        x = Dropout(0.1)(x)\n",
    "        \n",
    "        print(x.shape)\n",
    "        \n",
    "        x = TimeDistributed(GRU(32, activation='linear', return_sequences=True))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = MaxPool2D(data_format='channels_first')(x)\n",
    "        x = Dropout(0.1)(x)\n",
    "        \n",
    "        print(x.shape)\n",
    "        \n",
    "        x = Convolution2D(32, (4,10), padding=\"same\", data_format='channels_first')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = MaxPool2D(data_format='channels_first')(x)\n",
    "        x = Dropout(0.1)(x)\n",
    "        print(x.shape)\n",
    "        \n",
    "        x = Flatten()(x)\n",
    "        \n",
    "        x = Dense(64)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        out = Dense(nclass, activation=softmax)(x)\n",
    "\n",
    "        model = models.Model(inputs=inp, outputs=out)\n",
    "        opt = optimizers.Adam(config.learning_rate)\n",
    "\n",
    "        model = models.Model(inputs=inp, outputs=out)\n",
    "        \n",
    "        print(model.count_params())\n",
    "    \n",
    "        \n",
    "        opt = optimizers.Adam(self.learning_rate)\n",
    "\n",
    "        model.compile(optimizer=opt, loss=losses.categorical_crossentropy, metrics=['acc'])\n",
    "        return model\n",
    "    \n",
    "    def fit(self, X, Y, valX=None, valY=None, random_seed=1337):\n",
    "        if valX is None or valY is None:\n",
    "            trainX, valX, trainY, valY = train_test_split(X, Y, train_size=0.8, random_state=random_seed)\n",
    "        else:\n",
    "            trainX, trainY = X, Y\n",
    "        \n",
    "        checkpoint_filename = os.path.join(self.output_dir, 'best.h5')\n",
    "        \n",
    "        checkpoint = ModelCheckpoint(checkpoint_filename, \n",
    "                                     monitor='val_loss', verbose=1, save_best_only=True)\n",
    "        early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5)\n",
    "\n",
    "        callbacks_list = [checkpoint, early]\n",
    "        \n",
    "        model = self.model\n",
    "\n",
    "        history = model.fit(trainX, trainY, callbacks=callbacks_list, validation_data=(valX, valY), \n",
    "                            epochs=config.max_epochs, batch_size=64)\n",
    "\n",
    "        model.load_weights(checkpoint_filename)\n",
    "        \n",
    "        model_filename = os.path.join(self.output_dir, 'model.h5')\n",
    "        model.save(model_filename)\n",
    "\n",
    "        \n",
    "    def predict(self, X):\n",
    "        predictions = self.model.predict(X, batch_size=128)\n",
    "        return predictions\n",
    "        \n",
    "    def submission(self, X, fnames):\n",
    "        predictions = self.predict(X)\n",
    "        top_3 = np.array(LABELS)[np.argsort(-predictions, axis=1)[:, :3]]\n",
    "        predicted_labels = [' '.join(list(x)) for x in top_3]\n",
    "        \n",
    "        result = pd.DataFrame({'fname': fnames, 'label': predicted_labels})\n",
    "        result.set_index(\"fname\", inplace=True)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 1, 50, 63)\n",
      "(?, 32, 25, 31)\n",
      "(?, 32, 12, 16)\n",
      "(?, 32, 6, 8)\n",
      "(?, 32, 3, 16)\n",
      "(?, 32, 1, 8)\n",
      "113381\n"
     ]
    }
   ],
   "source": [
    "model = ModelRNN(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6062 samples, validate on 1516 samples\n",
      "Epoch 1/50\n",
      "6062/6062 [==============================] - 24s 4ms/step - loss: 3.5730 - acc: 0.0822 - val_loss: 3.4523 - val_acc: 0.1075\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.45227, saving model to output/ModelLSTM_y3oac/best.h5\n",
      "Epoch 2/50\n",
      "6062/6062 [==============================] - 10s 2ms/step - loss: 3.0275 - acc: 0.1826 - val_loss: 2.9101 - val_acc: 0.1926\n",
      "\n",
      "Epoch 00002: val_loss improved from 3.45227 to 2.91007, saving model to output/ModelLSTM_y3oac/best.h5\n",
      "Epoch 3/50\n",
      "6062/6062 [==============================] - 10s 2ms/step - loss: 2.6598 - acc: 0.2771 - val_loss: 2.6022 - val_acc: 0.2698\n",
      "\n",
      "Epoch 00003: val_loss improved from 2.91007 to 2.60222, saving model to output/ModelLSTM_y3oac/best.h5\n",
      "Epoch 4/50\n",
      "6062/6062 [==============================] - 11s 2ms/step - loss: 2.3719 - acc: 0.3435 - val_loss: 2.5182 - val_acc: 0.3100\n",
      "\n",
      "Epoch 00004: val_loss improved from 2.60222 to 2.51823, saving model to output/ModelLSTM_y3oac/best.h5\n",
      "Epoch 5/50\n",
      "6062/6062 [==============================] - 10s 2ms/step - loss: 2.1815 - acc: 0.3855 - val_loss: 2.1306 - val_acc: 0.3958\n",
      "\n",
      "Epoch 00005: val_loss improved from 2.51823 to 2.13056, saving model to output/ModelLSTM_y3oac/best.h5\n",
      "Epoch 6/50\n",
      "6062/6062 [==============================] - 10s 2ms/step - loss: 2.0402 - acc: 0.4294 - val_loss: 2.0527 - val_acc: 0.4017\n",
      "\n",
      "Epoch 00006: val_loss improved from 2.13056 to 2.05268, saving model to output/ModelLSTM_y3oac/best.h5\n",
      "Epoch 7/50\n",
      "6062/6062 [==============================] - 10s 2ms/step - loss: 1.9233 - acc: 0.4594 - val_loss: 1.9911 - val_acc: 0.4347\n",
      "\n",
      "Epoch 00007: val_loss improved from 2.05268 to 1.99106, saving model to output/ModelLSTM_y3oac/best.h5\n",
      "Epoch 8/50\n",
      "6062/6062 [==============================] - 10s 2ms/step - loss: 1.8390 - acc: 0.4749 - val_loss: 1.8959 - val_acc: 0.4525\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.99106 to 1.89587, saving model to output/ModelLSTM_y3oac/best.h5\n",
      "Epoch 9/50\n",
      "6062/6062 [==============================] - 10s 2ms/step - loss: 1.7563 - acc: 0.4975 - val_loss: 1.8215 - val_acc: 0.4723\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.89587 to 1.82149, saving model to output/ModelLSTM_y3oac/best.h5\n",
      "Epoch 10/50\n",
      "6062/6062 [==============================] - 10s 2ms/step - loss: 1.6884 - acc: 0.5180 - val_loss: 1.8006 - val_acc: 0.4888\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.82149 to 1.80062, saving model to output/ModelLSTM_y3oac/best.h5\n",
      "Epoch 11/50\n",
      "6062/6062 [==============================] - 10s 2ms/step - loss: 1.6375 - acc: 0.5294 - val_loss: 1.7388 - val_acc: 0.5066\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.80062 to 1.73884, saving model to output/ModelLSTM_y3oac/best.h5\n",
      "Epoch 12/50\n",
      "6062/6062 [==============================] - 10s 2ms/step - loss: 1.5809 - acc: 0.5452 - val_loss: 1.6808 - val_acc: 0.5356\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.73884 to 1.68079, saving model to output/ModelLSTM_y3oac/best.h5\n",
      "Epoch 13/50\n",
      "6062/6062 [==============================] - 10s 2ms/step - loss: 1.5259 - acc: 0.5587 - val_loss: 1.6494 - val_acc: 0.5343\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.68079 to 1.64942, saving model to output/ModelLSTM_y3oac/best.h5\n",
      "Epoch 14/50\n",
      "6062/6062 [==============================] - 10s 2ms/step - loss: 1.4763 - acc: 0.5701 - val_loss: 1.6082 - val_acc: 0.5501\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.64942 to 1.60823, saving model to output/ModelLSTM_y3oac/best.h5\n",
      "Epoch 15/50\n",
      "6062/6062 [==============================] - 10s 2ms/step - loss: 1.4570 - acc: 0.5711 - val_loss: 1.6542 - val_acc: 0.5442\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.60823\n",
      "Epoch 16/50\n",
      "6062/6062 [==============================] - 10s 2ms/step - loss: 1.4111 - acc: 0.5904 - val_loss: 1.6753 - val_acc: 0.5383\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 1.60823\n",
      "Epoch 17/50\n",
      "6062/6062 [==============================] - 10s 2ms/step - loss: 1.3631 - acc: 0.6044 - val_loss: 1.5520 - val_acc: 0.5673\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.60823 to 1.55200, saving model to output/ModelLSTM_y3oac/best.h5\n",
      "Epoch 18/50\n",
      "6062/6062 [==============================] - 10s 2ms/step - loss: 1.3171 - acc: 0.6183 - val_loss: 1.5665 - val_acc: 0.5712\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1.55200\n",
      "Epoch 19/50\n",
      "6062/6062 [==============================] - 11s 2ms/step - loss: 1.2875 - acc: 0.6219 - val_loss: 1.5327 - val_acc: 0.5752\n",
      "\n",
      "Epoch 00019: val_loss improved from 1.55200 to 1.53268, saving model to output/ModelLSTM_y3oac/best.h5\n",
      "Epoch 20/50\n",
      "6062/6062 [==============================] - 10s 2ms/step - loss: 1.2797 - acc: 0.6262 - val_loss: 1.4832 - val_acc: 0.6016\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.53268 to 1.48320, saving model to output/ModelLSTM_y3oac/best.h5\n",
      "Epoch 21/50\n",
      "6062/6062 [==============================] - 10s 2ms/step - loss: 1.2306 - acc: 0.6371 - val_loss: 1.5466 - val_acc: 0.5699\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 1.48320\n",
      "Epoch 22/50\n",
      "6062/6062 [==============================] - 10s 2ms/step - loss: 1.2072 - acc: 0.6366 - val_loss: 1.5489 - val_acc: 0.5798\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1.48320\n",
      "Epoch 23/50\n",
      "6062/6062 [==============================] - 10s 2ms/step - loss: 1.1723 - acc: 0.6536 - val_loss: 1.6169 - val_acc: 0.5745\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 1.48320\n",
      "Epoch 24/50\n",
      "6062/6062 [==============================] - 10s 2ms/step - loss: 1.1552 - acc: 0.6580 - val_loss: 1.5404 - val_acc: 0.5825\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 1.48320\n",
      "Epoch 25/50\n",
      "6062/6062 [==============================] - 10s 2ms/step - loss: 1.1169 - acc: 0.6631 - val_loss: 1.5495 - val_acc: 0.5825\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 1.48320\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train.reshape((-1, 1, X_train.shape[1], X_train.shape[2])), Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_submission = model.submission(X_test.reshape((-1, 1, X_test.shape[1], X_test.shape[2])), F_test)\n",
    "pred = np.asarray([x.split(' ') for x in y_submission.values[:, 0]])\n",
    "ans = train.label.loc[F_test].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7831134564643799"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(pred[:, 0] == ans) + np.mean(pred[:, 1] == ans) + np.mean(pred[:, 2] == ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
